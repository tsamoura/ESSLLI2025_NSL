# Neurosymbolic Learning: An Introductory Course to Theory and Applications

Neurosymbolic learning (NeSy) vows to transform AI by combining the strong induction capabilities of neural models with rigorous deduction from symbolic knowledge representation and reasoning techniques. This course focuses on various theoretical and practical aspects of NeSy. We will start by introducing a fundamental NSL problem, that of training neural models so that their predictions adhere to given sentences in propositional or first-order logic. We will then focus on a problem that has received substantial attention lately: training neural models using weak supervision, using supervision produced by logical theories. We will discuss necessary and sufficient conditions that ensure learnability under PAC semantics, the relationship between this problem and other problems in the weakly supervised machine learning literature, the phenomenon of learning imbalances, reasoning shortcuts and probabilistic reasoning. We will conclude this course by discussing problems related to scalability.

Program by day:
1. Introduction to NeSy
2. Learnability
3. Learning Imbalances in NeSy
4. Reasoning Shortcuts
5. Probabilistic Reasoning & Scaling
